# -*- coding: utf-8 -*-
"""Sentiment Analysis for Social Media.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1874fDd-SdBrgWN-aF2WFGW4uH1a_YFqA


#### Importing libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import nltk
import warnings
warnings.filterwarnings("ignore", category = DeprecationWarning)

# %matplotlib inline

"""#### Reading the train data:
Import the data using pandas and make a copy of the original data
"""

train = pd.read_csv('https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv')

train_orignal = train.copy()

"""#### Overview of the Training Data"""

train.head()

train.tail()

"""#### Reading the Test Data:
Import Data and backs up the original data
"""

test = pd.read_csv('https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/test.csv')

test_original = test.copy()

"""#### Overview of the test data:"""

test.head()

test.tail()

"""## Data Pre-processing

#### Combining the datasets
"""

combined_data = train.append(test,ignore_index=True,sort=True)
combined_data.head()

combined_data.tail()

"""#### Cleaning Data:
Removing the Usernames`(@)`
"""

def remove_pattern(text,pattern):

    # re.findall() finds the pattern in the text and will put it in a list
    r = re.findall(pattern,text)

    # re.sub() will substitute all the @ with an empty character
    for i in r:
        text = re.sub(i,"",text)

    return text

"""#### Making a column for the cleaned Tweets
Using regex and `np.vectorize()` for faster processing
"""

combined_data['Cleaned_Tweets'] = np.vectorize(remove_pattern)(combined_data['tweet'],"@[\w]*")

combined_data.head()

"""#### Now Removing punctuations, numbers and special characters

"""

combined_data['Cleaned_Tweets'] = combined_data['Cleaned_Tweets'].str.replace("[^a-zA-Z#]"," ")

combined_data.head()

"""#### Removing Short Words:
Words such as "hmm", "ok" etc. of length less than 3 are of no use
"""

combined_data['Cleaned_Tweets'] = combined_data['Cleaned_Tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))

combined_data.head()

"""### Tokenization:
Tokenizing the cleaned tweets as we will apply `Stemming` from `nltk`
"""

tokenized_tweets = combined_data['Cleaned_Tweets'].apply(lambda x: x.split())

tokenized_tweets.head()

"""### Stemming:
Step-based process of stripping the suffixes ("ing","ly",etc.) from a word
"""

from nltk import PorterStemmer

ps = PorterStemmer()

tokenized_tweets = tokenized_tweets.apply(lambda x: [ps.stem(i) for i in x])

tokenized_tweets.head()

"""#### Combining the data back:"""

for i in range(len(tokenized_tweets)):
    tokenized_tweets[i] = ' '.join(tokenized_tweets[i])

combined_data['Clean_Tweets'] = tokenized_tweets
combined_data.head()

"""### Data Visualization:

#### Visualize the data using WordCloud
"""

from wordcloud import WordCloud,ImageColorGenerator
from PIL import Image
import urllib
import requests

"""#### Storing all the non-sexist/racist words"""

positive_words = ' '.join(text for text in combined_data['Cleaned_Tweets'][combined_data['label'] == 0])

# Generating images
Mask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))

# Using ImageColorGenerator to generate the color of the image
image_color = ImageColorGenerator(Mask)

# Using WordCloud function of the wordcloud library
wc = WordCloud(background_color='black',height=1500,width=4000,mask=Mask).generate(positive_words)

# Size of the image generated
plt.figure(figsize=(10,20))

# Recoloring the words from the dataset to the image's color
# interpolation is used to smooth the image generated

plt.imshow(wc.recolor(color_func=image_color),interpolation="hamming")

plt.axis('off')
plt.show()

"""### Now lets store the words with label '1':

"""

negative_words = ' '.join(text for text in combined_data['Clean_Tweets'][combined_data['label'] == 1])

# Combining Image with Dataset
Mask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))

image_colors = ImageColorGenerator(Mask)

# Using WordCloud function from the wordcloud library
wc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(negative_words)

# Size of the image generated
plt.figure(figsize=(10,20))

# Recolor the words from the dataset to the image's color
# recolor just recolors the default colors to the image's blue color
# interpolation is used to smooth the image generated
plt.imshow(wc.recolor(color_func=image_colors),interpolation="gaussian")

plt.axis('off')
plt.show()

"""#### Now Extracting hastags from tweets:

"""

def extractHashtags(x):
    hashtags = []

    # Loop over the words in the tweet
    for i in x:
        ht = re.findall(r'#(\w+)',i)
        hashtags.append(ht)

    return hashtags

positive_hashTags = extractHashtags(combined_data['Cleaned_Tweets'][combined_data['label'] == 0])

positive_hashTags

"""#### Now unnesting the list:"""

positive_hastags_unnested = sum(positive_hashTags,[])
positive_hastags_unnested

"""#### Now storing the negative hastags:

"""

negative_hashtags = extractHashtags(combined_data['Cleaned_Tweets'][combined_data['label'] == 1])

negative_hashtags_unnest = (sum(negative_hashtags,[]))
negative_hashtags_unnest

"""### Plotting Bar Plots:

- Word Frequencies:
"""

positive_word_freq = nltk.FreqDist(positive_hastags_unnested)

positive_word_freq

"""#### Creating a dataframe of the most frequently used words in hashtags :"""

positive_df = pd.DataFrame({'Hashtags': list(positive_word_freq.keys()),'Count' : list(positive_word_freq.values())})
positive_df

"""#### Plotting the bar plot for 20 most frequent words:"""

positive_df_plot = positive_df.nlargest(20,columns='Count')

sns.barplot(data=positive_df_plot,y='Hashtags',x='Count')
sns.despine()

"""#### Negative Word Frequency:"""

negative_word_freq = nltk.FreqDist(negative_hashtags_unnest)

negative_word_freq

"""#### Creating a dataset of the most frequent words:"""

negative_df = pd.DataFrame({'Hashtags':list(negative_word_freq.keys()),'Count':list(negative_word_freq.values())})

negative_df

"""#### Plotting the bar plot for the 20 most frequent negative words:"""

negative_df_plot = negative_df.nlargest(20,columns='Count')

sns.barplot(data=negative_df_plot,y='Hashtags',x='Count')
sns.despine()

"""### Feature Extraction from Cleaned Tweets:

- Applying Bag of Words method to embed the data
- using `Count Vectorizer` package
"""

from sklearn.feature_extraction.text import CountVectorizer

bow_vecotrizer = CountVectorizer(max_df=0.90, min_df = 2, max_features = 1000, stop_words="english")

bow = bow_vecotrizer.fit_transform(combined_data['Cleaned_Tweets'])

bow_df = pd.DataFrame(bow.todense())

bow_df

"""### TF-IDF Features:

#### Term-Frequency (TF):
The first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document. The Term Frequency is calculated as:

![image.png](attachment:image.png)

#### Inverse-Document Frequency (IDF):
The second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears. The IDF is calulated as:

![image.png](attachment:image.png)

##### Now lets apply this to our dataset
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_df=0.90,min_df=2,max_features=1000,stop_words='english')

tfidf_matrix = tfidf.fit_transform(combined_data['Cleaned_Tweets'])

tfidf_df = pd.DataFrame(tfidf_matrix.todense())

tfidf_df

train_bow = bow[:31962]

train_bow.todense()

train_tfidf_matrix = tfidf_matrix[:31962]

train_tfidf_matrix.todense()

"""#### Splitting data into training data and test data:"""

from sklearn.model_selection import train_test_split

"""#### Bag of Words Features:"""

x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,train['label'],test_size=0.3,random_state=2)

"""#### TF-IDF Features:"""

x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,train['label'],test_size=0.3,random_state=17)

"""### Applying ML Models:"""

from sklearn.metrics import f1_score

"""#### Logistic Regression:"""

from sklearn.linear_model import LogisticRegression

log_Reg = LogisticRegression(random_state=0,solver='lbfgs')

"""### Fitting Bag of Words Features:"""

log_Reg.fit(x_train_bow,y_train_bow)

predict_bow = log_Reg.predict_proba(x_valid_bow)
predict_bow

"""#### Calculating the F1-Score:"""

# If prediction is more than or equal to 0.3 then 1 else 0
prediction_int = predict_bow[:,1] >=0.3

# Converting to integer type
prediction_int = prediction_int.astype(np.int)
prediction_int

# Calculating f1 score
log_bow = f1_score(y_valid_bow, prediction_int)
log_bow

"""### Fitting TF-IDF Features:"""

log_Reg.fit(x_train_tfidf,y_train_tfidf)

predict_tfidf = log_Reg.predict_proba(x_valid_tfidf)
predict_tfidf

prediction_int = predict_tfidf[:,1]>=0.3

prediction_int = prediction_int.astype(np.int)
prediction_int

log_tfidf = f1_score(y_valid_tfidf,prediction_int)
log_tfidf

"""### Predicting the test_data and storing it:"""

test_tfidf = tfidf_matrix[31962:]
test_pred = log_Reg.predict_proba(test_tfidf)

test_pred_int = test_pred[:,1] >= 0.3
test_pred_int = test_pred_int.astype(np.int)

test['label'] = test_pred_int

submission = test[['id','label']]
submission.to_csv('result.csv', index=False)

"""## Results after prediction:
### For a negative label : 1
### For a positive label : 0
"""

res = pd.read_csv('result.csv')
res

"""### Summary:
**F-1 Score of Model: 0.5315391084945332 (Bag of Words) & 0.5558534405719392 (TF-IDF) using Logistic Regression**
"""